{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽取文章所属频道关键词向量特征\n",
    "def extract_channel_keyword_feature(partition):\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "    \n",
    "    for row in partition:\n",
    "        try:\n",
    "            weights = sorted([row.preference_info[key] for key in row.preference_info.keys()\n",
    "                             if key.split(':')[0] == row.channel_id], reverse=True)[:10]\n",
    "        except Exception as e:\n",
    "            print(e.message)\n",
    "            weights = [0.0] * 10\n",
    "        yield row.article_id, row.channel_id, row.user_id, int(row.gender), int(row.age), \\\n",
    "            Vectors.dense(weights if weights else [0.0] * 10), row.click_flag\n",
    "\n",
    "# 抽取文章关键词向量特征\n",
    "def extract_feature(partition):\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "    for row in partition:\n",
    "        try:\n",
    "            weights = sorted(row.keywords.values(), reverse=True)[:10]\n",
    "        except Exception as e:\n",
    "            print(e.message)\n",
    "            weights = [0.0] * 10\n",
    "        yield row.article_id, Vectors.dense(weights if weights else [0.0] * 10)\n",
    "\n",
    "def array_to_vector(partition):\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "    for row in partition:\n",
    "        yield row.article_id, Vectors.dense(row.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "def vector_to_double(row):\n",
    "    return float(row.click_flag), float(row.probability[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成用户召回结果\n",
    "def gen_lr_sort_model():\n",
    "    self.spark_session.sql(\"use portal\")\n",
    "    # 用户文章点击行为\n",
    "    # userId, articalId, channelId, clickFlag\n",
    "    # 用户ID, 文章ID, 频道Id, 是否点击\n",
    "    sql = \"select user_id, article_id, channel_id, click_flag from t_user_behaviour\"\n",
    "    user_article_click_df = self.spark_session.sql(sql)\n",
    "#     user_article_click_df.show()\n",
    "\n",
    "    # 获取用户画像\n",
    "    # userId, gender, age, preference\n",
    "    # 用户ID, 性别, 年龄, 偏好\n",
    "    sql = \"select split(user_id, ':')[1] user_id, basic_info.gender, basic_info.age, preference_info \" \\\n",
    "          \"from t_user_profile\"\n",
    "    user_profile_df = self.spark_session.sql(sql)\n",
    "#     user_profile_df.show()\n",
    "    # 连接用户画像和实时特征\n",
    "    user_article_click_df = user_article_click_df.join(user_profile_df, on=[\"user_id\"], how=\"left\")\n",
    "    # 向量化\n",
    "    user_article_click_df = user_article_click_df.rdd.mapPartitions(extract_channel_keyword_feature) \\\n",
    "        .toDF([\"article_id\", \"channel_id\", \"user_id\", \"gender\", \"age\", \"channel_weights\", \"click_flag\"])\n",
    "#     user_article_click_df.show()\n",
    "\n",
    "    # 获取文章画像\n",
    "    article_profile_df = self.spark_session.sql(\"select * from t_article_profile\")\n",
    "\n",
    "\n",
    "    article_profile_df = article_profile_df.rdd.mapPartitions(extract_feature).toDF([\"article_id\", \"article_weights\"])\n",
    "#     article_profile_df.show()\n",
    "\n",
    "    user_article_click_df = user_article_click_df.join(article_profile_df, on=[\"article_id\"], how=\"inner\")\n",
    "#     user_article_click_df.show()\n",
    "\n",
    "    # 获取文章向量\n",
    "    article_vector_df = self.spark_session.sql(\"select article_id, vector from t_article_vector\")\n",
    "\n",
    "    article_vector_df = article_vector_df.rdd.mapPartitions(array_to_vector).toDF([\"article_id\", \"article_vector\"])\n",
    "#     article_vector_df.show()\n",
    "\n",
    "    user_article_click_df = user_article_click_df.join(article_vector_df, on=[\"article_id\"], how=\"inner\")\n",
    "#     user_article_click_df.show()\n",
    "\n",
    "    # 收集特征\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    input_cols = [\"channel_id\", \"gender\", \"age\", \"channel_weights\", \"article_weights\", \"article_vector\"]\n",
    "    user_article_click_df = VectorAssembler().setInputCols(input_cols) \\\n",
    "                                             .setOutputCol(\"features\") \\\n",
    "                                             .transform(user_article_click_df)\n",
    "    user_article_click_df.show()\n",
    "\n",
    "    # Logistic Regression\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    logistic_regression = LogisticRegression()\n",
    "    logistic_regression_model = logistic_regression.setFeaturesCol(\"features\") \\\n",
    "                                                   .setLabelCol(\"click_flag\")\\\n",
    "                                                   .fit(user_article_click_df)\n",
    "    logistic_regression_model.write().overwrite().save(\n",
    "        \"hdfs://192.168.0.1:9000/user/models/logistic_regression/lr.model\")\n",
    "\n",
    "    from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "    logistic_regression_model = LogisticRegressionModel.load(\n",
    "        \"hdfs://192.168.0.1:9000/user/models/logistic_regression/lr.model\")\n",
    "    logistic_regression_result = logistic_regression_model.transform(user_article_click_df)\n",
    "    logistic_regression_result.select([\"click_flag\", \"probability\", \"prediction\"]).show()\n",
    "\n",
    "\n",
    "    score_labels = logistic_regression_result.select([\"click_flag\", \"probability\"]).rdd.map(vector_to_double)\n",
    "    score_labels.collect()\n",
    "    \n",
    "    from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "    \n",
    "    binary_classification_metrics = BinaryClassificationMetrics(scoreAndLabels=score_labels)\n",
    "    area_under_roc = binary_classification_metrics.areaUnderROC\n",
    "    print(area_under_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_lr_sort_model_metrics(test_df):\n",
    "    from pyspark.ml.classification import LogisticRegressionModel\n",
    "    logistic_regression_model = LogisticRegressionModel.load(\n",
    "        \"hdfs://192.168.0.1:9000/user/models/logistic_regression/lr.model\")\n",
    "    lr_result = logistic_regression_model.evaluate(test_df).predictions\n",
    "    lr_result.show()\n",
    "\n",
    "    def vector_to_double(row):\n",
    "        return float(row.click_flag), float(row.probability[1])\n",
    "    score_labels = lr_result.select([\"click_flag\", \"probability\"]).rdd.map(vector_to_double)\n",
    "    score_labels.collect()\n",
    "\n",
    "    from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "    binary_classification_metrics = BinaryClassificationMetrics(scoreAndLabels=score_labels)\n",
    "    area_under_roc = binary_classification_metrics.areaUnderROC\n",
    "    print(area_under_roc)\n",
    "\n",
    "    tp = lr_result[(lr_result.click_flag == 1) & (lr_result.prediction == 1)].count()\n",
    "    tn = lr_result[(lr_result.click_flag == 0) & (lr_result.prediction == 1)].count()\n",
    "    fp = lr_result[(lr_result.click_flag == 0) & (lr_result.prediction == 1)].count()\n",
    "    fn = lr_result[(lr_result.click_flag == 1) & (lr_result.prediction == 0)].count()\n",
    "    print(\"tp {} tn {} fp {} fn {}\".format(tp, tn, fp, fn))\n",
    "    print('accuracy is : %f' % ((tp + tn) / (tp + tn + fp + fn)))\n",
    "    print('recall is : %f' % (tp / (tp + fn)))\n",
    "    print('precision is : %f' % (tp / (tp + fp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_sort_model.gen_lr_sort_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
