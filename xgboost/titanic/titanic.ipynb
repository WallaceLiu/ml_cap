{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言\n",
    "建议再看一遍电源《泰坦尼克号》，可能会给你一些启发，比如妇女儿童先上船等，所以是否获救其实并非随机，而是基于一些背景有先后顺序的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 背景介绍\n",
    "1912年4月15日，载着1316号乘客和891名船员的豪华巨轮泰坦尼克号在首次航行期间撞上冰山后沉没，2224名乘客和机组人员中有1502人遇难。沉船导致大量伤亡的原因之一是没有足够的救生艇给乘客和船员。虽然幸存下来有一些运气因素，但有一些人比其他人更有可能生存，比如妇女，儿童和上层阶级。在本文中将对哪些人可能生存作出分析，特别是运用Python和机器学习的相关模型工具来预测哪些乘客幸免于难，最后提交结果。\n",
    "\n",
    "其中，训练和测试数据是一些乘客的个人信息以及存活状况，要尝试根据它生成合适的模型并预测其他人的存活状况。这是一个二分类的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 数据文件说明\n",
    "从Kaggle泰坦尼克号项目页面下载数据：https://www.kaggle.com/c/titanic\n",
    "\n",
    "对于上述变量的说明：\n",
    "- PassengerID（ID）\n",
    "- Survived(存活与否)\n",
    "- Pclass（客舱等级，在当时的英国阶级分层比较严重，较为重要）\n",
    "- Name（姓名，可提取出更多信息）\n",
    "- Sex（性别，较为重要）\n",
    "- Age（年龄，较为重要）\n",
    "- Parch（直系亲友，是指父母，孩子，其中1表示有一个，依次类推）\n",
    "- SibSp（旁系，是指兄弟姐妹）\n",
    "- Ticket（票编号，这个是个玄学问题）\n",
    "- Fare（票价，可能票价贵的获救几率大）\n",
    "- Cabin（客舱编号）\n",
    "- Embarked（上船的港口编号，是指从不同的港口上船）\n",
    "\n",
    "每个乘客有12个属性，其中，PassengerID在这里只起到索引作用，而Survived是我们要预测的目标，因此我们要处理的数据总共有10个变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.361582</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>13.019697</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.361582    0.523008   \n",
       "std     257.353842    0.486592    0.836071   13.019697    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   35.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据中发现，总数为891个，那还有特征是有空值的，Cabin甚至只有一点数据（不着急，这里我们先看看）。而且有些数据是数值型的，一些是文本型的，还有一些是类目性的。\n",
    "大概有 0.383838的人最终获救了，平均乘客年龄大概是29.88岁等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill age with median\n",
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "# Embarked\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna('S')\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "titanic = pandas.read_csv(\"/Users/cap/data/titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_splits=3, random_state=1, shuffle=True)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf.split(titanic):\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14029180695847362\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7946127946127947\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1,max_iter=200)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = model_selection.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_test = pandas.read_csv(\"/Users/cap/data/titanic/test.csv\")\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8069584736251403\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "kf = model_selection.KFold(titanic.shape[0], n_splits=3, random_state=1, shuffle=True)\n",
    "scores = model_selection.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8282828282828283\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "kf = model_selection.KFold(titanic.shape[0], n_splits=3, random_state=1, shuffle=True)\n",
    "scores = model_selection.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Major         2\n",
      "Mlle          2\n",
      "Col           2\n",
      "Don           1\n",
      "Mme           1\n",
      "Lady          1\n",
      "Countess      1\n",
      "Jonkheer      1\n",
      "Sir           1\n",
      "Capt          1\n",
      "Ms            1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))\n",
    "\n",
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"NameLength\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=8, min_samples_leaf=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13692480359147025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\",]],\n",
    "    [LogisticRegression(random_state=1,max_iter=200), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_splits=3, random_state=1, shuffle=True)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf.split(titanic):\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pandas.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11603675, 0.47300453, 0.12511163, 0.13057681, 0.51998898,\n",
       "       0.14410782, 0.63987486, 0.18128421, 0.67801121, 0.12091831,\n",
       "       0.12061032, 0.21154569, 0.91153109, 0.10806179, 0.89177584,\n",
       "       0.87819921, 0.16491598, 0.13917736, 0.54000998, 0.55187228,\n",
       "       0.22447455, 0.53708143, 0.90693218, 0.39533285, 0.88077072,\n",
       "       0.10285529, 0.91057254, 0.13743394, 0.31382623, 0.12622768,\n",
       "       0.11607906, 0.18421566, 0.54881314, 0.49382821, 0.42899459,\n",
       "       0.14225062, 0.5085549 , 0.52452288, 0.13239476, 0.28233455,\n",
       "       0.1107034 , 0.47244675, 0.09915256, 0.83507153, 0.90025673,\n",
       "       0.14955007, 0.31895892, 0.13749447, 0.89017572, 0.53809975,\n",
       "       0.36226886, 0.17952188, 0.83392161, 0.87848669, 0.17773741,\n",
       "       0.13687091, 0.10597667, 0.12315896, 0.12054873, 0.912202  ,\n",
       "       0.13130534, 0.15463645, 0.13016598, 0.66596429, 0.66143891,\n",
       "       0.87353969, 0.67310315, 0.29012002, 0.35830872, 0.84817467,\n",
       "       0.66219768, 0.12700062, 0.55254144, 0.3738702 , 0.91071794,\n",
       "       0.41013552, 0.12969613, 0.83733447, 0.15766189, 0.66219768,\n",
       "       0.68139871, 0.19987298, 0.20581139, 0.12061032, 0.18756475,\n",
       "       0.13086423, 0.656308  , 0.53078422, 0.65424435, 0.80332024,\n",
       "       0.53658369, 0.1206007 , 0.89333249, 0.12969613, 0.29124575,\n",
       "       0.12317171, 0.86360351, 0.14617581, 0.58618403, 0.12196764,\n",
       "       0.90502326, 0.14872764, 0.13749447, 0.12228643, 0.62285947,\n",
       "       0.13087586, 0.14630437, 0.13749447, 0.12974897, 0.17830323,\n",
       "       0.14315752, 0.6542502 , 0.89690016, 0.67214584, 0.87959316,\n",
       "       0.14012212, 0.11764327, 0.69883008, 0.3697836 , 0.86318492,\n",
       "       0.87881494, 0.12577649, 0.90365978, 0.12055893, 0.13749447,\n",
       "       0.57011138, 0.12599202, 0.63598523, 0.13349464, 0.13317959,\n",
       "       0.12662938, 0.51380831, 0.23655287, 0.10759838, 0.09813168,\n",
       "       0.12408839, 0.1331372 , 0.16430056, 0.51998994, 0.12211925,\n",
       "       0.20756433, 0.90536313, 0.19190003, 0.1630415 , 0.4325603 ,\n",
       "       0.10453682, 0.34170404, 0.13508994, 0.47244675, 0.34251505,\n",
       "       0.9150408 , 0.13171606, 0.10604076, 0.48619965, 0.11209167,\n",
       "       0.12406094, 0.9104007 , 0.57961939, 0.4325603 , 0.51087502,\n",
       "       0.65424101, 0.57879537, 0.82303961, 0.12053907, 0.28603853,\n",
       "       0.58367622, 0.30305811, 0.14596189, 0.90359231, 0.52240082,\n",
       "       0.12058279, 0.1326349 , 0.1239809 , 0.1316531 , 0.13174191,\n",
       "       0.87548536, 0.87791662, 0.29721625, 0.8334426 , 0.8535534 ,\n",
       "       0.15766189, 0.33582402, 0.90339289, 0.13749447, 0.91676202,\n",
       "       0.13624926, 0.85718667, 0.12262455, 0.14080858, 0.1358879 ,\n",
       "       0.13543715, 0.26175113, 0.49874448, 0.12624353, 0.72286548,\n",
       "       0.10731735, 0.85710227, 0.59040029, 0.16921473, 0.53801173,\n",
       "       0.64776481, 0.66412527, 0.60479695, 0.87654843, 0.16562142,\n",
       "       0.26360866, 0.62896172, 0.16740003, 0.89155129, 0.12318046,\n",
       "       0.12766658, 0.12054304, 0.24869335, 0.79777208, 0.41090849,\n",
       "       0.30046246, 0.65427023, 0.21562212, 0.89837764, 0.12969613,\n",
       "       0.8155285 , 0.13608939, 0.84413046, 0.12699073, 0.87844102,\n",
       "       0.59607263, 0.12502302, 0.65424435, 0.11397652, 0.14484434,\n",
       "       0.25357202, 0.89454213, 0.11623743, 0.13750883, 0.34478123,\n",
       "       0.12802569, 0.19253306, 0.14038111, 0.81277371, 0.89753274,\n",
       "       0.87640675, 0.82436178, 0.32954886, 0.12060966, 0.33079201,\n",
       "       0.28950847, 0.88017439, 0.16039803, 0.86318492, 0.58948859,\n",
       "       0.74977015, 0.15439995, 0.39862584, 0.13329498, 0.12637346,\n",
       "       0.12058279, 0.13749447, 0.12969613, 0.83231388, 0.12698868,\n",
       "       0.1082775 , 0.12699651, 0.85105402, 0.65218268, 0.16807489,\n",
       "       0.12061032, 0.22542659, 0.12058279, 0.5085549 , 0.14046417,\n",
       "       0.34617766, 0.13749447, 0.91587777, 0.63228917, 0.13165271,\n",
       "       0.85939561, 0.16050429, 0.12501358, 0.14379191, 0.1710652 ,\n",
       "       0.52012299, 0.66310377, 0.65424435, 0.64319371, 0.71444822,\n",
       "       0.10529833, 0.12055893, 0.36751729, 0.1316531 , 0.12969613,\n",
       "       0.33880539, 0.59224126, 0.1316531 , 0.5021599 , 0.12007342,\n",
       "       0.12229668, 0.78209363, 0.12622768, 0.33536984, 0.1198033 ,\n",
       "       0.11753661, 0.17764027, 0.12157908, 0.13323497, 0.65424435,\n",
       "       0.8206552 , 0.33569351, 0.67795537, 0.20845969, 0.42040796,\n",
       "       0.13933815, 0.13800687, 0.12058478, 0.6167977 , 0.90001385,\n",
       "       0.67476614, 0.23626249, 0.176827  , 0.12143675, 0.18697792,\n",
       "       0.12228643, 0.1346902 , 0.16430056, 0.46059837, 0.90526843,\n",
       "       0.12495368, 0.86884018, 0.34705456, 0.14584392, 0.17341615,\n",
       "       0.8190449 , 0.33248577, 0.13165271, 0.64279415, 0.12144017,\n",
       "       0.25696679, 0.15456263, 0.09318082, 0.21135413, 0.35139083,\n",
       "       0.17881598, 0.11752292, 0.14684166, 0.91340362, 0.33460407,\n",
       "       0.61915337, 0.16430056, 0.62132161, 0.16792723, 0.85286141,\n",
       "       0.89649859, 0.16562142, 0.24648691, 0.15992792, 0.70327069,\n",
       "       0.15823917, 0.85601579, 0.120609  , 0.13749447, 0.56995593,\n",
       "       0.10367994, 0.87778548, 0.86977742, 0.13057681, 0.92016465,\n",
       "       0.1549835 , 0.13086455, 0.53193639, 0.89615756, 0.17557998,\n",
       "       0.15588962, 0.90921588, 0.16574033, 0.13123293, 0.87571432,\n",
       "       0.90850079, 0.4883931 , 0.17313684, 0.19904942, 0.13488743,\n",
       "       0.13749447, 0.13985428, 0.53933239, 0.59435361, 0.16088762,\n",
       "       0.8338991 , 0.12408124, 0.11942715, 0.14629485, 0.18771445,\n",
       "       0.3902307 , 0.87802933, 0.56347553, 0.12783344, 0.10292726,\n",
       "       0.9128133 , 0.1422557 , 0.88776832, 0.12599003, 0.12910898,\n",
       "       0.90759102, 0.12661769, 0.91043421, 0.36732288, 0.30758334,\n",
       "       0.19341033, 0.15251827, 0.26369214, 0.6542385 , 0.64845545,\n",
       "       0.65424435, 0.90750507, 0.56816414, 0.12969613, 0.85999049,\n",
       "       0.10050122, 0.12969613, 0.41610677])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1,max_iter=200), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
