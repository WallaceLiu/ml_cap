{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost提供了gradient boosting的一个高效实现，可以训练random forest ensembles.\n",
    "\n",
    "Random forest比gradient boosting简单. The XGBoost library allows the models to be trained in a way that repurposes and harnesses the computational efficiencies implemented in the library for training random forest models.\n",
    "\n",
    "本文使用XGBoost训练random forest ensembles.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "- XGBoost provides an efficient implementation of gradient boosting that can be configured to train random forest ensembles.\n",
    "- 如何使用XGBoost训练和评估random forest ensemble模型解决分类和回归问题.\n",
    "- 如何调优XGBoost random forest ensemble模型超参."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Random Forest With XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an open-source library that provides an efficient implementation of the gradient boosting ensemble algorithm, referred to as Extreme Gradient Boosting or XGBoost for short.\n",
    "\n",
    "As such, XGBoost refers to the project, the library, and the algorithm itself.\n",
    "\n",
    "Gradient boosting是分类和回归预测模型的首选算法，因为其性能是最好的. 通常，gradient boosting训练模型很慢, and the problem is exasperated by large datasets.\n",
    "\n",
    "XGBoost addresses the speed problems of gradient boosting by introducing a number of techniques that dramatically accelerate the training of the model and often result in better overall performance of the model.\n",
    "\n",
    "更多关于XGBoost参考: [A Gentle Introduction to XGBoost for Applied Machine Learning](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)\n",
    "\n",
    "除了支持gradient boosting, XGBoost也支持其他tree ensemble算法, 如random forest.\n",
    "\n",
    "Random forest is an ensemble of decision trees algorithms.\n",
    "\n",
    "Each decision tree is fit on a [bootstrap sample](https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/) of the training dataset. This is a sample of the training dataset where a given example (rows) may be selected more than once, referred to as sampling with replacement.\n",
    "\n",
    "Importantly, a random subset of the input variables (columns) at each split point in the tree is considered. This ensures that each tree added to the ensemble is skillful, but different in random ways. The number of features considered at each split point is often a small subset. For example, on classification problems, a common heuristic is to select the number of features equal to the square root of the total number of features, e.g. 4 if a dataset had 20 input variables.\n",
    "    \n",
    "更多关于random forest ensemble参考: [How to Develop a Random Forest Ensemble in Python](https://machinelearningmastery.com/random-forest-ensemble-in-python/)\n",
    "\n",
    "用XGBoost训练random forest的主要好处是速度快. It is expected to be significantly faster to use than other implementations, such as the native scikit-learn implementation.\n",
    "\n",
    "Now that we know that XGBoost offers support for the random forest ensemble, let’s look at the specific API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. XGBoost API for Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost library provides two wrapper classes that allow the random forest implementation provided by the library to be used with the scikit-learn machine learning library.\n",
    "\n",
    "`XGBRFClassifier`和`XGBRFRegressor`类分别针对分类和回归问题.\n",
    "\n",
    "```\n",
    "...\n",
    "# define the model\n",
    "model = XGBRFClassifier()\n",
    "```\n",
    "\n",
    "The number of trees used in the ensemble can be set via the “n_estimators” argument, and typically, this is increased until no further improvement in performance is observed by the model. Often hundreds or thousands of trees are used.\n",
    "\n",
    "```\n",
    "...\n",
    "# define the model\n",
    "model = XGBRFClassifier(n_estimators=100)\n",
    "```\n",
    "XGBoost does not have support for drawing a bootstrap sample for each decision tree. This is a limitation of the library.\n",
    "\n",
    "Instead, a subsample of the training dataset, without replacement, can be specified via the “subsample” argument as a percentage between 0.0 and 1.0 (100 percent of rows in the training dataset). Values of 0.8 or 0.9 are recommended to ensure that the dataset is large enough to train a skillful model but different enough to introduce some diversity into the ensemble.\n",
    "\n",
    "```\n",
    "...\n",
    "# define the model\n",
    "model = XGBRFClassifier(n_estimators=100, subsample=0.9)\n",
    "```\n",
    "\n",
    "The number of features used at each split point when training a model can be specified via the “colsample_bynode” argument and takes a percentage of the number of columns in the dataset from 0.0 to 1.0 (100 percent of input rows in the training dataset).\n",
    "\n",
    "If we had 20 input variables in our training dataset and the heuristic for classification problems is the square root of the number of features, then this could be set to sqrt(20) / 20, or about 4 / 20 or 0.2.\n",
    "\n",
    "```\n",
    "...\n",
    "# define the model\n",
    "model = XGBRFClassifier(n_estimators=100, subsample=0.9, colsample_bynode=0.2)\n",
    "```\n",
    "\n",
    "更多配置XGBoost random forest ensembles参考: [Random Forests in XGBoost](https://xgboost.readthedocs.io/en/latest/tutorials/rf.html)\n",
    "\n",
    "Now that we are familiar with how to use the XGBoost API to define random forest ensembles, let’s look at some worked examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. XGBoost Random Forest for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at developing an XGBoost random forest ensemble for a classification problem.\n",
    "\n",
    "首先, we can use the [make_classification() function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) to create a synthetic binary classification problem with 1,000 examples and 20 input features.\n",
    "\n",
    "The complete example is listed below.\n",
    "\n",
    "```\n",
    "# test classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)\n",
    "```\n",
    "\n",
    "Running the example creates the dataset and summarizes the shape of the input and output components.\n",
    "\n",
    "接下来, 可以在该数据集上用XGBoost random forest算法.\n",
    "\n",
    "We will evaluate the model using repeated [stratified k-fold cross-validation](https://machinelearningmastery.com/cross-validation-for-imbalanced-classification/), with three repeats and 10 folds. We will report the mean and standard deviation of the accuracy of the model across all repeats and folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n",
      "Mean Accuracy: 0.832 (0.036)\n"
     ]
    }
   ],
   "source": [
    "# evaluate xgboost random forest algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from xgboost import XGBRFClassifier\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)\n",
    "# define the model\n",
    "model = XGBRFClassifier(n_estimators=100, subsample=0.9, colsample_bynode=0.2)\n",
    "# define the model evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example reports the mean and standard deviation accuracy of the model.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we can see the XGBoost random forest ensemble achieved a classification accuracy of about 89.1 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the XGBoost random forest model as a final model and make predictions for classification.\n",
    "\n",
    "First, the XGBoost random forest ensemble is fit on all available data, then the predict() function can be called to make predictions on new data.\n",
    "\n",
    "The example below demonstrates this on our binary classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "# make predictions using xgboost random forest for classification\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_classification\n",
    "from xgboost import XGBRFClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# define the model\n",
    "model = XGBRFClassifier(n_estimators=100, subsample=0.9, colsample_bynode=0.2)\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# define a row of data\n",
    "row = [0.2929949,-4.21223056,-1.288332,-2.17849815,-0.64527665,2.58097719,0.28422388,-7.1827928,-1.91211104,2.73729512,0.81395695,3.96973717,-2.66939799,3.34692332,4.19791821,0.99990998,-0.30201875,-4.43170633,-2.82646737,0.44916808]\n",
    "row = asarray([row])\n",
    "# make a prediction\n",
    "yhat = model.predict(row)\n",
    "# summarize the prediction\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上面示例在整个数据集上拟合XGBoost random forest ensemble 模型，再用一个新数据做预测, 之后，我们就可以在应用中使用该模型."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with using random forest for classification, let’s look at the API for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. XGBoost Random Forest for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at developing an XGBoost random forest ensemble for a regression problem.\n",
    "\n",
    "首先, 使用[make_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) 函数伪造回归问题的数据集，1000条，20个特征.\n",
    "\n",
    "完整示例如下."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# test regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates the dataset and summarizes the shape of the input and output components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来, 我们在该数据集上使用XGBoost random forest ensemble.\n",
    "\n",
    "As we did with the last section, we will evaluate the model using repeated k-fold cross-validation, with three repeats and 10 folds.\n",
    "\n",
    "We will report the mean absolute error (MAE) of the model across all repeats and folds. The scikit-learn library makes the MAE negative so that it is maximized instead of minimized. This means that larger negative MAE are better and a perfect model has a MAE of 0.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepeatedKFold(n_repeats=3, n_splits=10, random_state=1)\n",
      "MAE: -137.932 (7.413)\n"
     ]
    }
   ],
   "source": [
    "# evaluate xgboost random forest ensemble for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRFRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
    "# define the model\n",
    "model = XGBRFRegressor(n_estimators=100, subsample=0.9, colsample_bynode=0.2)\n",
    "# define the model evaluation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example reports the mean and standard deviation MAE of the model.\n",
    "\n",
    "Note: Your [results may vary](https://machinelearningmastery.com/different-results-each-time-in-machine-learning/) given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we can see the random forest ensemble with default hyperparameters achieves a MAE of about 108."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the XGBoost random forest ensemble as a final model and make predictions for regression.\n",
    "\n",
    "首先, random forest ensemble拟合所有可用的数据, 再调用`predict()`函数在新数据上预测.\n",
    "\n",
    "The example below demonstrates this on our regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:50:11] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Prediction: 21\n"
     ]
    }
   ],
   "source": [
    "# gradient xgboost random forest for making predictions for regression\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_regression\n",
    "from xgboost import XGBRFRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
    "# define the model\n",
    "model = XGBRFRegressor(n_estimators=100, subsample=0.9, colsample_bynode=0.2)\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# define a single row of data\n",
    "row = [0.20543991,-0.97049844,-0.81403429,-0.23842689,-0.60704084,-0.48541492,0.53113006,2.01834338,-0.90745243,-1.85859731,-1.02334791,-0.6877744,0.60984819,-0.70630121,-1.29161497,1.32385441,1.42150747,1.26567231,2.56569098,-0.11154792]\n",
    "row = asarray([row])\n",
    "# make a prediction\n",
    "yhat = model.predict(row)\n",
    "\n",
    "# summarize the prediction\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上面示例在整个数据集上拟合XGBoost random forest ensemble 模型，再用一个新数据做预测, 之后，我们就可以在应用中使用该模型."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with how to develop and evaluate XGBoost random forest ensembles, let’s look at configuring the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. XGBoost Random Forest 超参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will take a closer look at some of the hyperparameters you should consider tuning for the random forest ensemble and their effect on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索树的个数\n",
    "树的个数是配置XGBoost random forest的另一个关键的超参.\n",
    "\n",
    "通常, 增加树的个数直到模型性能稳定. Intuition might suggest that more trees will lead to overfitting, although this is not the case. Both bagging and random forest algorithms appear to be somewhat immune to overfitting the training dataset given the stochastic nature of the learning algorithm.\n",
    "\n",
    "树的个数可以通过设置 “n_estimators” 参数，默认为 100.\n",
    "\n",
    "The example below explores the effect of the number of trees with values between 10 to 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">10 0.824 (0.037)\n",
      ">50 0.834 (0.036)\n",
      ">100 0.832 (0.036)\n",
      ">500 0.835 (0.035)\n",
      ">1000 0.835 (0.036)\n",
      ">5000 0.833 (0.033)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# explore xgboost random forest number of trees effect on performance\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from xgboost import XGBRFClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\treturn X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\t# define the number of trees to consider\n",
    "\tn_trees = [10, 50, 100, 500, 1000, 5000]\n",
    "\tfor v in n_trees:\n",
    "\t\tmodels[str(v)] = XGBRFClassifier(n_estimators=v, subsample=0.9, colsample_bynode=0.2)\n",
    "\treturn models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t# define the model evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate the model\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\t# evaluate the model and collect the results\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\t# store the results\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\t# summarize performance along the way\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the mean accuracy for each configured number of trees.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "In this case, we can see that performance rises and stays flat after about 500 trees. Mean accuracy scores fluctuate across 500, 1,000, and 5,000 trees and this may be statistical noise.\n",
    "\n",
    "A box and whisker plot is created for the distribution of accuracy scores for each configured number of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索特征的个数\n",
    "The number of features that are randomly sampled for each split point is perhaps the most important feature to configure for random forest.\n",
    "\n",
    "It is set via the “colsample_bynode” argument, which takes a percentage of the number of input features from 0 to 1.\n",
    "\n",
    "The example below explores the effect of the number of features randomly selected at each split point on model accuracy. We will try values from 0.0 to 1.0 with an increment of 0.1, although we would expect values below 0.2 or 0.3 to result in good or best performance given that this translates to about the square root of the number of input features, which is a common heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0.1 0.839 (0.035)\n",
      ">0.2 0.832 (0.036)\n",
      ">0.3 0.836 (0.033)\n",
      ">0.4 0.831 (0.039)\n",
      ">0.5 0.827 (0.037)\n",
      ">0.6 0.819 (0.037)\n",
      ">0.7 0.812 (0.038)\n",
      ">0.8 0.807 (0.037)\n",
      ">0.9 0.796 (0.038)\n",
      ">1.0 0.792 (0.038)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# explore xgboost random forest number of features effect on performance\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import arange\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from xgboost import XGBRFClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\treturn X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tfor v in arange(0.1, 1.1, 0.1):\n",
    "\t\tkey = '%.1f' % v\n",
    "\t\tmodels[key] = XGBRFClassifier(n_estimators=100, subsample=0.9, colsample_bynode=v)\n",
    "\treturn models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t# define the model evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate the model\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\t# evaluate the model and collect the results\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\t# store the results\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\t# summarize performance along the way\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box and whisker plot is created for the distribution of accuracy scores for each feature set size.\n",
    "\n",
    "We can see a trend in performance decreasing with the number of features considered by the decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "\n",
    "## Tutorials\n",
    "- [A Gentle Introduction to XGBoost for Applied Machine Learning](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)\n",
    "- [How to Develop a Random Forest Ensemble in Python](https://machinelearningmastery.com/random-forest-ensemble-in-python/)\n",
    "- [Gradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost](https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/)\n",
    "- [How to Develop Your First XGBoost Model in Python with scikit-learn](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/)\n",
    "\n",
    "## APIs\n",
    "- [Random Forests in XGBoost](https://xgboost.readthedocs.io/en/latest/tutorials/rf.html)\n",
    "- [xgboost.XGBRFClassifier API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRFClassifier)\n",
    "- [xgboost.XGBRFRegressor API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRFRegressor)\n",
    "\n",
    "## Summary\n",
    "In this tutorial, you discovered how to use the XGBoost library to develop random forest ensembles.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "- XGBoost provides an efficient implementation of gradient boosting that can be configured to train random forest ensembles.\n",
    "- How to use the XGBoost API to train and evaluate random forest ensemble models for classification and regression.\n",
    "- How to tune the hyperparameters of the XGBoost random forest ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
