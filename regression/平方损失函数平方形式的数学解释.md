# 由线性模型引出平方损失函数
假设有一个房屋销售的数据，如表所示：
面积`$m^2$`|售价（万元）
---|---
123|250
150|320
87|160
102|220
...|...
将上面数据可视化如：

![图1](http://note.youdao.com/yws/api/personal/file/2C9356C568BF4086B1013BBCE6765582?method=download&shareKey=c866c0a500e0d3dcfbbf0c50807a81f7)

那么，如果有一个新房屋，如何通过房屋面积预测其房价？

我们可以用一条曲线尽量拟合这些数据点，当有新的数据(房屋面积)时，就可以得到对应值(房屋价格)。

如果用一条直线拟合，可能是下面这个图像：

![图2](http://note.youdao.com/yws/api/personal/file/FD88FD504D924E33A517CECC8397495D?method=download&shareKey=2f9020ebb06b901ddd7273cffa80463f)

用sklearn实现，如下代码所示：
```
from matplotlib.lines import Line2D
import matplotlib.pyplot as plt
from sklearn import linear_model
import numpy as np

reg = linear_model.LinearRegression()
X = np.array([[123, 0], [150, 0], [87, 0], [102, 0]])
y = np.array([250, 320, 160, 220])
reg.fit(X, y)

figure, ax = plt.subplots()

plt.scatter(X[:, 0], y)

print(reg.coef_)
print(reg.intercept_)

line1 = [(87, reg.intercept_ + reg.coef_[0] * 87), (150, reg.intercept_ +
                                                    reg.coef_[0] * 150)]
(line1_xs, line1_ys) = zip(*line1)
ax.add_line(Line2D(line1_xs, line1_ys, linewidth=1, color='blue'))

plt.plot()
plt.show()
```
上图蓝线就是我们想要得到的直线，这是个线性模型。线性回归假设特征和结果满足线性关系。

线性关系看上去简单，但表达能力非常强，在自然界普遍存在。每个特征对结果的影响强弱可以由前面的参数体现。

一般化的线性模型，是有多个变量的线性模型。如果用向量形式来表达，如下：
```math
h_\theta=\theta^Tx
```
求出不同的`$\theta$`可以得到不同的线性模型。

问题是，我们如何知道一个模型的好坏！

这就引出了损失函数，如下：
```math
J(\theta)=\frac{1}{2}\sum_{i=1}^n(h_\theta(x^{(i)}-y^{(i)})^2

\min_{\theta} J_\theta
```
通过最小化损失函数得到的`$\theta$`，就是我们觉得最优的`$\theta$`。

那么，损失函数为什么是平方形式呢？如果只是表达模型预测出来的值与真实值无限逼近，三次方，四次方……哪怕绝对值应该也可以，如下所示：
```math
J(\theta)=\frac{1}{2}\sum_{i=1}^n(h_\theta(x^{(i)}-y^{(i)})^3

J(\theta)=\frac{1}{2}\sum_{i=1}^n(h_\theta(x^{(i)}-y^{(i)})^4
```
可为什么是平方形式呢？这就是下面要说明的。

#  平方形式的数学解释
假设根据特征的预测结果与实际结果有误差`$\epsilon^{(i)}$`，那么预测结果`$\theta^Tx^{(i)}$`与真实结果`$y^{(i)}$`，满足下面公式：
```math
y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}
```
实际中，训练数据都是海量的，根据中心极限定理，我们可以假定误差满足平均值u为0的正态分布`$N(0,\alpha^2)$`。

那么x，y的条件概率满足如下：
```math
p(y^{(i)}|x^{(i)}:\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
```
上式相当于估计了一条样本的结果概率，根据极大似然估计，我们希望在给定的训练数据中，给定的训练数据出现的概率（可能性）最大，也就是概率积最大！累积形式如下：
```math
L(u,\sigma^2)=\prod_{i=1}^N\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(\epsilon_i-u)^2}{2\sigma^2})
```
取对数：
```math
\log L(u,\sigma^2)=-\frac{n}{2}\log\sigma^2-\frac{n}{2}\log 2\pi-\frac{\sum_{i=1}^n(\epsilon_i-u)^2}{2\sigma^2}
```
我们想让`$\log L(u,\sigma^2)$`最大，这些训练数据的误差方差我们假设是一个定值（数据趋于无穷大的时候，均值与方差都是可以看成定值），也就是对求似然估计没有影响。

因此，若想让
```math
\log L(u,\sigma^2)
```
最大，只要让
```math
\frac{1}{2}\sum_{i=1}^n(h_\theta(x^{(i)}-y^{(i)})^2
```
最小即可，其他均为常量。