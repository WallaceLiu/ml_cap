{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# ## 1）环境准备\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from os import path, listdir\n",
    "from sklearn.datasets import load_svmlight_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "print tf.__version__\n",
    "print tf.__path__\n",
    "\n",
    "\n",
    "# ## 2）数据准备Dataset格式\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\"\"\"\n",
    "解析CSV格式，对输入的每一行样本，进行格式解析，返回labels和dense_vector格式数据\n",
    "例如输入csv格式字符串： 0.0,0.6666666666666666,0.5,0.0,0.0,0.0,0.0,0.7272727272727273,0.42857142857142855\n",
    "\"\"\"\n",
    "# 数据处理\n",
    "def process_data(data_type, my_path, feature_size, batch_size=32, num_epochs=1):\n",
    "    filenames = get_file_list(my_path)\n",
    "    next_element = read_my_file_format(data_type, filenames, feature_size, batch_size, num_epochs)\n",
    "    return next_element\n",
    "\n",
    "# 创建session，指定GPU或者CPU使用率\n",
    "def get_session(gpu_fraction=0.1):\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction,\n",
    "                                allow_growth=True)\n",
    "    return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "# 测试数据\n",
    "filenames = '/data/all-csv'\n",
    "feature_size = 530\n",
    "batch_size = 3\n",
    "num_epochs = 1\n",
    "data_type = 'csv'\n",
    "next_element = process_data(data_type, filenames, feature_size, batch_size, num_epochs)\n",
    "print next_element['dense_vector']\n",
    "print next_element['labels']\n",
    "\n",
    "gpu_fraction = 0.2\n",
    "my_device='/gpu:0'\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer())\n",
    "with tf.device(my_device):\n",
    "    sess = get_session(gpu_fraction)\n",
    "    sess.run(init_op)\n",
    "    dense_vector, labels = sess.run([next_element['dense_vector'],next_element['labels']])\n",
    "    print dense_vector\n",
    "    print labels\n",
    "\n",
    "\n",
    "# ## 3）LR模型\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "class LR(object):\n",
    "    \"\"\" 初始化成员变量 \"\"\"\n",
    "    def __init__(self, feature_size, loss_fuc, train_optimizer, learning_rate, reg_type, reg_param):\n",
    "        # 特征向量长度\n",
    "        self.feature_size = feature_size\n",
    "        # 损失函数\n",
    "        self.loss_fuc = loss_fuc\n",
    "        # 优化方法\n",
    "        self.train_optimizer = train_optimizer\n",
    "        # 学习率\n",
    "        self.learning_rate = learning_rate\n",
    "        # 正则类型\n",
    "        self.reg_type = reg_type\n",
    "        # 正则因子\n",
    "        self.reg_param = reg_param\n",
    "        # aglobal_step\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "        \n",
    "    def train(self, batch_data):\n",
    "        \"\"\" 1 定义输入数据 \"\"\"\n",
    "        with tf.name_scope('input_data'):\n",
    "            # 标签：[batch_size, 1]\n",
    "            labels = batch_data['labels']\n",
    "            # 用户特征向量：[batch_size, feature_size]\n",
    "            dense_vector = tf.reshape(batch_data['dense_vector'], shape=[-1, feature_size, 1]) # None * feature_size * 1\n",
    "            print(\"%s: %s\" % (\"dense_vector\", dense_vector))\n",
    "            print(\"%s: %s\" % (\"labels\", labels))\n",
    "            \n",
    "        \"\"\" 2 定义网络输出 \"\"\"\n",
    "        with tf.name_scope(\"LR_Comput_Score\"):\n",
    "            # LR参数，生成或者获取w b\n",
    "            with tf.variable_scope(\"lr_layer\", reuse=tf.AUTO_REUSE):\n",
    "                self.w = tf.get_variable(name='w', shape=[self.feature_size, 1], initializer=tf.glorot_normal_initializer())\n",
    "                self.b = tf.get_variable(name='bias', shape=[1], initializer=tf.constant_initializer(0.0))\n",
    "            print(\"%s: %s\" % (\"w\", self.w))\n",
    "            print(\"%s: %s\" % (\"b\", self.b))\n",
    "            \n",
    "            # ---------- w * x  + b----------   \n",
    "            Y_first = tf.reduce_sum(tf.multiply(self.w, dense_vector), 2)  # None * F\n",
    "            print(\"%s: %s\" % (\"Y_first\", Y_first))\n",
    "            # ---------- sum(w * x)  + b----------   \n",
    "            Y_Out = tf.reduce_sum(Y_first, 1)\n",
    "            Y_bias = self.b * tf.ones_like(Y_Out, dtype=tf.float32) # None * 1\n",
    "            print(\"%s: %s\" % (\"Y_bias\", Y_bias))\n",
    "            Y_Out = tf.add(Y_Out, Y_bias, name='Y_Out') \n",
    "            print(\"%s: %s\" % (\"Y_Out\", Y_Out))\n",
    "            # ---------- score ----------  \n",
    "            score=tf.nn.sigmoid(Y_Out,name='score')\n",
    "            score=tf.reshape(score, shape=[-1, 1])\n",
    "            print(\"%s: %s\" % (\"score\", score))\n",
    "        \n",
    "        \"\"\" 3 定义损失函数和AUC指标 \"\"\"\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # loss：Squared_error，Cross_entropy ,FTLR\n",
    "            if reg_type == 'l1_reg':\n",
    "                regularization = self.reg_param * tf.reduce_sum(tf.abs(self.w))\n",
    "#                 tf.contrib.layers.l1_regularizer(self.reg_param)(self.w) \n",
    "            elif reg_type == 'l2_reg':\n",
    "                regularization = self.reg_param * tf.nn.l2_loss(self.w) \n",
    "            else:  \n",
    "                regularization = self.reg_param * tf.nn.l2_loss(self.w)                 \n",
    "            \n",
    "            if loss_fuc == 'Squared_error':\n",
    "                loss = tf.reduce_mean(tf.reduce_sum(tf.square(labels - score), reduction_indices=[1])) + regularization\n",
    "            elif loss_fuc == 'Cross_entropy':\n",
    "                loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(Y_Out, [-1]), labels=tf.reshape(labels, [-1]))) + regularization\n",
    "            elif loss_fuc == 'FTLR':\n",
    "                loss = tf.reduce_mean(tf.reduce_sum(tf.square(labels - score), reduction_indices=[1])) + regularization\n",
    "            # AUC                  \n",
    "            auc = tf.metrics.auc(labels, score)\n",
    "            print(\"%s: %s\" % (\"labels\", labels))\n",
    "            # w为0的比例,w的平均值\n",
    "            w_zero_ratio = tf.reduce_mean(tf.to_float(tf.abs(self.w) <= 1.0e-5))\n",
    "            w_avg = tf.reduce_mean(self.w)\n",
    "            \n",
    "        \"\"\" 4 设定optimizer \"\"\"\n",
    "        with tf.name_scope(\"optimizer\"):\n",
    "            with tf.variable_scope(\"optimizer\", reuse=tf.AUTO_REUSE):\n",
    "                #------bulid optimizer------\n",
    "                if train_optimizer == 'Adam':\n",
    "                    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "                elif train_optimizer == 'Adagrad':\n",
    "                    optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate, initial_accumulator_value=1e-8)\n",
    "                elif train_optimizer == 'Momentum':\n",
    "                    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n",
    "                elif train_optimizer == 'ftrl':\n",
    "                    optimizer = tf.train.FtrlOptimizer(learning_rate)\n",
    "                train_step = optimizer.minimize(loss, global_step=self.global_step)               \n",
    "\n",
    "        \"\"\"5 设定summary，以便在Tensorboard里进行可视化 \"\"\"\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "            tf.summary.scalar(\"accumulate_auc\", auc[0])\n",
    "            tf.summary.scalar(\"w_avg\", w_avg)\n",
    "            tf.summary.scalar(\"w_zero_ratio\", w_zero_ratio)\n",
    "            tf.summary.histogram(\"w\", self.w)\n",
    "            # 好几个summary，所以这里要merge_all\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            \n",
    "        \"\"\"6 返回结果 \"\"\"\n",
    "        return Y_Out, score, regularization, loss, auc, train_step, w_zero_ratio, w_avg, labels, score, summary_op\n",
    "\n",
    "\n",
    "# ## 4）模型训练测试\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "# 数据准备  \n",
    "filenames = '/data/csv-all'\n",
    "data_type='csv'\n",
    "feature_size = 530\n",
    "batch_size = 60000\n",
    "num_epochs = 200\n",
    "next_element = process_data(data_type, filenames, feature_size, batch_size, num_epochs)\n",
    "    \n",
    "# 模型参数   \n",
    "loss_fuc = 'Squared_error'\n",
    "train_optimizer = 'Adam'\n",
    "learning_rate = 0.01\n",
    "reg_type = 'l2_reg'\n",
    "reg_param = 0.0\n",
    "log_path='/data/log/Squared_error_lr_L2_0_20180816_01'\n",
    "\n",
    "# 开始训练\n",
    "bea_model = LR(feature_size, loss_fuc, train_optimizer, learning_rate, reg_type, reg_param)\n",
    "Y_Out, score, regularization, loss, auc, train_step, w_zero_ratio, w_avg, labels, score, summary_op = bea_model.train(next_element)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer())\n",
    "gpu_fraction = 0.4\n",
    "my_device='/gpu:1'\n",
    "with tf.device(my_device):\n",
    "    sess = get_session(gpu_fraction)\n",
    "    sess.run(init_op)\n",
    "    batch_cnt = 0\n",
    "    #选定可视化存储目录\n",
    "    writer = tf.summary.FileWriter(log_path, sess.graph)\n",
    "    try:\n",
    "         while True:\n",
    "            batch_cnt = batch_cnt + 1           \n",
    "            a, b, c, d, e, summary = sess.run([loss, auc, w_zero_ratio, w_avg, train_step, summary_op])\n",
    "            if batch_cnt % 50 == 0 or batch_cnt <= 10:\n",
    "                y, p = sess.run([labels, score])\n",
    "                if y.sum() > 0.0:\n",
    "                    batch_auc=metrics.roc_auc_score(y, p)\n",
    "                else:\n",
    "                    batch_auc=0.0\n",
    "                print(\"batch: {} loss: {:.4f} accumulate_auc: {:.4f} batch_auc: {:.4f} w_zero_ratio: {:.4f} w_avg: {:.4f}\".format(batch_cnt, a, b[0], batch_auc, c, d))\n",
    "                writer.add_summary(summary, batch_cnt)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"3、Train end of dataset\")   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
